<h1><span>a. Что было сделано?</span></h1>

1. Парсинг веб-сайта: Был выполнен сбор данных. В частности, собирались следующие элементы:
   - Цитаты (<span class="text">)
   - Авторы цитат (<small class="author">)
   - Теги, относящиеся к каждой цитате (<meta class="keywords" content="...">)
   
2. Структурирование данных: Все собранные данные были организованы в формат JSON, который позволяет легко хранить и передавать структурированные данные.

3. Сохранение результатов: Данные были записаны в файл quotes.json. Этот файл содержит список объектов, где каждая запись представляет собой одну цитату вместе с её автором и тегами.

<h1><span>b. Откуда были получены данные?</span></h1>

Данные были получены с сайта https://quotes.toscrape.com/. 

<h1><span>с. Как осуществлялся сбор?</span></h1>

Сбор данных осуществлялся следующим образом:

1. Сначала была отправлена HTTP-запрос через библиотеку requests на каждую страницу сайта, начиная с первой.
2. Полученный HTML-код обрабатывался с помощью библиотеки BeautifulSoup, которая позволяет удобно извлекать нужные элементы из разметки.
3. На каждой странице находили блоки с классом quote, внутри которых находились цитата, автор и теги.
4. Эти данные собирали в словарь, который затем добавляли в общий список.
5. Процесс повторялся для каждой следующей страницы, пока не заканчивались доступные страницы.
6. Наконец, собранные данные преобразовывались в формат JSON и сохранялись в файл.

<h1><span>d. Почему был выбран тот или иной метод/инструмент, а не другой?</span></h1>

 Мой выбор инструментов:

1. Библиотека requests:
   - Эта библиотека используется для отправки HTTP-запросов и получения содержимого веб-страницы. Она проста в использовании и поддерживает различные методы запросов (GET, POST и др.), а также обработку cookies и заголовков.
   
2. Библиотека BeautifulSoup:
   - Одна из самых популярных библиотек для парсинга HTML и XML документов. Она позволяет легко находить и извлекать необходимые элементы из HTML-разметки благодаря использованию CSS-селекторов и других методов поиска.
   
3. Модуль json:
   - Стандартный модуль Python для работы с форматом JSON. Позволяет сериализовать объекты Python в строки JSON и наоборот. Удобен для хранения и передачи данных между различными системами.

Причины выбора метода:

1. Автоматизация процесса сбора данных:
   Сбор данных вручную может занять много времени и быть подверженным ошибкам. Использование автоматических средств, таких как написанный скрипт, позволяет значительно ускорить процесс и минимизировать вероятность ошибок.

2. Универсальность подхода:
   Выбранные инструменты позволяют собирать данные практически с любого сайта, поддерживающего стандартные протоколы HTTP и предоставляющего HTML-документы. Это делает решение гибким и применимым в разных ситуациях.

3. Простота и доступность:
   Все используемые библиотеки являются стандартными для Python и широко применяются в сообществе разработчиков. Они хорошо документированы и имеют обширную поддержку, что облегчает разработку и сопровождение решения.

